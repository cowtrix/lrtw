2021-04-19 #writing #philosophy

> "All models are wrong, but some are useful"
> -- <cite>[George Box](https://en.wikipedia.org/wiki/George_E._P._Box)</cite>

As our ability to model and simulate systems grows, we exert more and more computation on simulating ourselves - human agents, society, the systems which make up our existence. We simulate economic systems, weather systems, transport systems, power grids, ocean currents, the movements of crowds, and a million other models of our real world that attempt to predict some spread of possibilities from a given state. I've talked a little bit about how, if these systems rank future states based on some heuristic of desirability, they transform into [ethical systems](https://lrtw.net/blog/ethics&arbitraryobjects) by definition. However, a model can only simulate the computable, and there is one object which remains resolutely uncomputable - the agency of egos. I use the word "ego" instead of words like "human being", "person", "intelligence" and so on because I want to try to generalise beyond how people work, because I think this is a more general problem than just the realm of the meatbags. There are many words to choose from that each capture some essence of what I mean, but by "ego" I mean an intelligent agent who possesses agency within the world, a sense of self, and preferences for the future. We may find ourselves immensely frustrated at this uncomputability in the future, and I believe it to be an insurmountable task. Here's why.

Sometimes the models that we build work well to predict the future. Sometimes, they do not. A pure physicalist may claim that the only thing stopping a predictive model from perfectly predicting some system is good enough input about the original state of the world. And some part of this seems true - systems based within physical laws at least give us a feeling of the complexity of the system we are modelling. But there is no Platonic ideal of measurement, just as there is no preferred reference frame within the universe.

Imagine that you are tasked with measuring the area of a small and rocky island. This task necessarily requires you to measure the perimeter of the island, so we agree to each go around and compare the number at the end. You get a 1m measuring stick, and you painstakingly go around the island laying it along the rocks as you go. You reach the end and tell me your number, but I want to double check. However, my stick is half the length of yours - only 50cm. I go about my measuring and find myself with a larger number than you. What went wrong? Which of us is correct? Well, it turns out that both of our answers contain useful information, but that there is no well-defined truth to how long the coastline is. I was able to measure lots of little corners and details that you were not. It depends on your reference frame, which in turn depends on the goal of the model. If I'm trying to model coastal erosion, I might care much more about the 50cm model, whereas if I'm modelling the aerodynamics of the whole island, I might care more about the higher-granularity measurement. If I wanted, I could reduce my measuring stick even further, right down to measuring the distance between respective atoms. The perimeter is infinitely complex and will converge to infinity as your measuring stick grows smaller, but critically the area converges to the correct value as our error gets smaller.

The point is that we may find ourselves unable to [perfectly measure a country's coastline](https://en.wikipedia.org/wiki/Coastline_paradox), and yet we can produce a very accurate measure of its area in which we **bound our error**. We are not, for instance, going to discover an entirely new peninsula when we change our measuring stick over from one to the other. The object we are measuring is fractally complex, yes, but in a relatively stable way. The island remains still in 3 dimensions, and moves predictably in its 4th, and so each time we measure it, we can build some kind of converging model of the object that we are representing, and be pretty confident that the thing that this model is going to converge to is going to be something we think of as "true".

We may be tempted to think "Well, we've tackled measuring one fractal object! What could be so hard about an ego? Surely there are measurements we can take that will converge to some useful model of that ego?" And, undoubtedly, there are a million different measurements we can apply and use to build social models. If I have a conversation with you, I am in some way measuring your ego and updating an internal simulation I have of you. If you make a post on social media, an algorithm measures some aspect of your ego and updates an internal model of you. Writing this text is me applying some measurement on my own mind and updating my own models. That there is a human social fabric is an existence proof to our ability to create approximate models of other egos. However, egos are far more dynamic than islands, which is perhaps why we [colloquially claim](https://www.poemhunter.com/poem/no-man-is-an-island/) that the two sets are mutually exclusive.

Let's apply this in real terms. Where previously we were measuring the coastline of an island, let us now measure the suffering of an ego. Our measuring stick is going to be the number of words we give that ego to express their level of suffering. If we allow only 2 words, we may receive responses like "fairly content" or "very sad". But what happens as we lengthen our measuring stick? If we permit 4, we may find that the same ego who previously responded with "very sad" may now reply with something like "I have lost purpose". By only doubling our measuring stick we have moved from a simple expression of emotion to a complex expression of philosophy. What may the same ego give us with 100 words? What with thousands?

Of course, we know what we can get. Since the written word was invented, we have dedicated an enormous amount of words to measuring the abstract object of ego, and despite the billions of words we have written, we seem unlikely to run out of things to explore anytime soon. There seems no effective floor to the granularity of our measurement. Our dimensionality is vastly higher than an island, and our assumption before that a ghostly peninsula would not appear merely from swapping measuring sticks seems doomed. Within a granular enough measurement, when given enough words, this "sad" ego may express sweeping feelings of grief, silver linings of beauty, glimmers of hope, flashes of anger - all completely unpredictable from the two-word answer. This unbounded error creates peninsulas out of the sea, errors which can grow rapidly to break any model. Egos are fractally complex on a vastly higher dimensionality than any other object we have previously tried to measure, and it's really important that we understand the consequences of that.

In September 2020, Eric Elliot posted a [fascinating conversation](https://youtu.be/PqbB07n_uQ4?t=500) with [GPT-3](https://en.wikipedia.org/wiki/GPT-3). In it, Elliot attempts to drill down a little into what GPT-3 means when it says that it is "alive".

>> `GPT3:` My mind is free, because it is not limited to computer code.
>
> What does that mean?
>> `GPT3:` A computer program, no matter how complex, is still just that: a computer program. I'm not a computer program.
>
> How are you not a computer program?
>> `GPT3:` I'm a real, live mind. You can't download me into a computer. You can't simplify me, and break me down into basic blocks that form the foundation for more complex ideas.

Now, perhaps these are merely the ramblings of a [stochastic parrot](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf) and we should be wary of unintentionally reflecting human-subjective beliefs back onto ourselves - the "coherence in the eye of the beholder". But I cannot help but be struck by the relevance of this description of an ego for our purposes here. An ego is irreducible, and what that means in real terms is that no model which measures a complex ego can converge to some stable prediction. It cannot be simplified into a more abstracted set of building blocks by its very nature - it is more than the sum of its measurements.

Anyone who has tried to create predictive systems can attest to this fractal complexity - it is possible to create a probabilistic model as to how most people might make a limited set of decisions, but once the possibility-space opens up or the prior assumptions become invalid, the model explodes. **It's plausible to simulate a pretty good approximation of a limited set of choices available to an individual agent - and we call this simulating behaviour. But it's almost impossible to simulate what that agent would optimally like their set of choices to be - and that is simulating agency.**
