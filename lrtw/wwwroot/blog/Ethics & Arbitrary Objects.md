2021-02-25 #writing #philosophy

There are many abstract, mental systems that we build throughout our lives. We build systems in order to interpret our memories and sensory input into an internal simulation of "reality", our experience up to that moment. Their second function is then to run simulations of the future and predict their probability 

These systems are abstract objects that exist within your mind that define your experience within the world.

However, at some point, your systems will come to the final part of this process of existence: selecting which predicted future you should pursue. This will involve using some form of heuristic to determine future value, and then balancing that value against probability.

But what is this heuristic exactly? Consider the plethora of future states that lay before you, running the full gamut from your worst fears to your wildest dreams. If, right now, you could choose which future to inhabit with perfect information about the outcomes of each branch of the timeline, which mental system would you choose to evaluate this choice? What would you call it?

I've thought about this a lot these last few months, and about that question. I propose that:

> A system of ethics is any system which predicts future states of the universe and values them.

I think that's an interesting definition to explore. It avoids anthropocentrism, is an objective description of the subjective, and captures the essential quality of that system. But that's not going to be my boldest proposal. That's going to be the following:

> All ethical heuristics are arbitrary.

What exactly do I mean when I say arbitrary? Specifically here I mean that there is no correlation between the evaluation function of future timelines and their probability. Put another way, the evaluation function does not exist or emerge from any physical laws of the universe.

As far as I can tell, the universe contains no preference between future states that you find undesirable and states you find desirable. The laws of physics contains no preference between an Earth that remains habitable and one that is smacked with a world-killer asteroid in a century. My personal preference between those future states is arbitrary - and, critically, because that valuation is arbitrary it is by definition an ethical one.

I cannot stress enough that when I describe these preferences between future states as arbitrary, I do not mean that they are unimportant. In fact, I mean exactly the opposite. These arbitrary structures determine the shape of our existence and our world. All that can be inferred, measured, and determined by reason is solid and non-arbitrary. But reason does not rank future states - only an arbitrary mechanism can do that. Empirical measures may enter into the determination of action, but they cannot ultimately value them.

Why is this important? Because it means that you *cannot* build a full set of philosophical systems using entirely reason, logic, empiricism and science. Any set of philosophical systems must contain an arbitrary object, which we call ethics. You can certainly create that arbitrary object with the assistance of some external authority like religion or popular opinion, but I think that just kicks the arbitrary can down the road.