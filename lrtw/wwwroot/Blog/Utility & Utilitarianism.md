2021-06-10 #writing #philosophy

Utilitarianism is an ethical system based upon maximising or minimising some utility function. A utility function, in this sense, is some operation on a set of information that produces a value. In other words, it is a function that makes statements like "state `A` of the universe is less desirable than state `B`". In conversations around human ethics, this utilitarian utility function is often vaguely expressed in terms of "well-being" or "happiness". On occasion, I've been called a utilitarian because of my discussions of heuristics, ethical systems as [any systems which rank the future](https://lrtw.net/blog/ethics&arbitraryobjects), and ["maximizing" or "minimizing" things](https://lrtw.net/blog/whydemocracy), but I'm not quite sure I understand what it means to be one.

It seems to me that *all* ethical systems contain utility functions, by their very nature of being information systems which rank future states of the universe. Someone driven by a religious ethical system might take their utility function from an ancient holy text. Someone driven by egoism centers their utility function entirely on their own pleasures and desires. A nihilist may choose to implement an entirely random utility function. All of these ethical choices are driven by some concept of utility. What then distinguishes these systems from the label of utilitarianism?

# Human Utility

When we learn about agents, we learn about an increasing hierarchy of inner complexity. A reflexive agent merely responds to stimuli with no internal process or perception of the world. A model-reflexive agent stores some internal information about the world around it, but still follows simple reflexive rules. A goal-driven agent will attempt to change the world around it to meet some kind of given criteria. And finally, the most complex of agents are utility-driven, and that is the category that most mammals find themselves within. A utility -driven agent will select goals that maximize some arbitrary utility function. I think it is accurate to describe the majority of mammal utility functions - like that of a dog, a bear, a dolphin - as trying to achieve satisfaction, safety, and happiness.

These utility functions could be hypothesized to be homogenous across a mammalian species. The things that make one pig happy are fairly likely to make another entirely distant pig just as happy on average. Because of the homogeneity of the function, we can produce a general model that will apply well to all present and future pigs. It is therefore relatively trivial to produce an eternal, utopian society for a bunch of pigs, if one were so inclined and had the material resources. It is doubtful that such a utopia would be interesting to us, and I imagine it would involve many brushy forest floors full of berries and bushes that never rustle with the presence of a predator.

The human utility function, however, is a far more strange and diverse class. Some may claim that it's about "being happy" but I'm not sure that holds true. Certainly happiness is a common utility function, but it's a complicated word and people often choose to pursue other things than happiness. We can certainly argue that these people do have happiness as their utility function, and they're just irrational agents who are not acting to maximize it. But I think the realms of higher consciousness hold an undeniable restless quality to them. The human relaxes, is content, but then thinks - *now what?*

# Model vs. Consensus

[As we've explored previously](https://lrtw.net/blog/egosarefractallycomplex), it may not be possible to make models of a conscious mind - and therefore the exact nature of that internal "now what" moment - that are generalizable, abstract and accurate. This is a problem for our utopia-obsessed society-designers. The utility functions of humans are both *heterogenous* - in that each is unique and incongruous with all other functions - and *non-fungible* - in that one utility function's valuation cannot be easily measured against any other utility function's valuation with some non-arbitrary measure. Because of these two qualities, building an abstract model that can tell us how to maximize the utility function across a group of humans seems doomed to fail. It's like trying to create a universal plug that fits into billions of different sockets, when every socket is slightly different from every other one. Even if we were capable of building such a model, it is likely that it would require such a vast amount of information about our lives that we would lose all sense of privacy. Such a model would be extremely vulnerable to [Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law), and will likely produce people gaming whatever abstract metric emerges. It is here that I think I find my primary objection to a particular flavour of utilitarianism, that I will call *model* utilitarianism. This is when we try to build an abstract model of an ethical situation, and derive our utility function from that model.


But this is not the only place from which we can derive such a utility function. Instead of trying to capture reality and derive a function from it, we can instead directly measure the function from within the environment. *Consensus* utilitarianism distinguishes itself by a rejection of any abstract model of conscious preference. Within personal ethics, this means that you should try to gather as many perspectives as you can to influence your choices, and you should value other people's perspectives with the same function by which you value your own perspective. This should not lead us into blind populism, where an action is only good if a majority of the agents you know about support it. But it should at least warn us that we will never be able to understand exactly *how*, in an abstract and general sense, to maximize a utility function across an entire civilization.

Consensus utilitarianism may seem, when phrased politically, to tie fairly neatly [into our previous discussions of the virtues of radical democracy](https://lrtw.net/blog/whydemocracy). Model utilitarianism is the realm of the think-tank, the planning committee, the advisory report - sweeping plans for a better world built upon abstractions and containing predictions of conscious agency. These simplified worlds are useful sandboxes within which to play, and they have their place within the complex art of evaluating future states of the universe. But at the end of the day there is no way to reliably predict the suffering you will inflict on others through your choices without communicating with them about their subjective experience. It is simply not possible to know how to act as a good person, and how to build a better world, without understanding the consensus around you.